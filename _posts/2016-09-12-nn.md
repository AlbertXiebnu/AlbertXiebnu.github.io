---
layout: post
title: "神经网络"
modified:
categories: 
excerpt:
tags: []
comments: true
author: xiexiaobo
image:
  feature: sample-image-4.jpg
date: 2016-09-12T15:37:07+08:00
---

神经网络可以说是深度学习的基础。从最早接触神经网络到现在已经有比较长的时间了。但是神经网络最核心的部分，梯度的后向传播算法（back propagation)可能是最难的部分，因为涉及到一些矩阵微积分的知识。我自己也是恶补了一下这方面的东西。虽然网上也有很多非常好的神经网络的博客（见最后参考部分），但是在这篇博客中，我希望从数学角度，一步一步的呈现神经网络的后向传播算法的推导，同时也实现了一个非常简单的神经网络模型（不是很高效的实现）。

## 模型结构
首先来看一个最简单的三层神经网络。它是有输入层，隐藏层和输出层构成。图中每个圆圈表示一个神经元。其中输入层神经元的个数等于输入向量$\bf{x}$的维度。输出层神经元的个数取决于你的问题。如果是分类问题，则等于类别的个数 。例如如果你需要构建一个神经网络来区分手写数字0-9，则输出层神经元个数等于10，第$i$个神经元的输出表示样本属于数字$i$的概率。

![three-layer network](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/nn-from-scratch-3-layer-network.png)

中间蓝色部分称为隐藏层。在这个例子中，只有一层的隐藏层，事实上，隐藏层的层数没有限制，你可以添加很多的隐藏层，事实上，隐藏层的数量越多，网络参数越多，模型也越复杂，训练也越困难，因为梯度在后向传播的过程中衰减非常的快。当然，隐藏层越多，神经网络的非线性拟合能力越强（可以把神经网络看成是一个函数映射：$\bf{f}:\bf{x} -> \bf{y}$。深度网络一般有多个隐层，因此学习能力很强。

## 前向传播

首先用数学符号简单定义神经网络。我用$\mathbf{net}(p,q,r)$表示3层神经网络，输入层、隐藏层和输出层的维数分别是p,q,r。因此输入向量为$\mathbf{x}\in \mathbb{R}^{1\times p}$（注意和一些文章的记号不同的是，这里的输入向量是一个$1\times p$的行向量），模型的输出向量记为$\mathbf{\hat y}\in \mathbb{R}^{1\times r}$。输入层到隐藏层的参数矩阵记为$\mathbf{W_1}\in \mathbb{R}^{p\times q}$，bias记为$\mathbf{b_1}\in \mathbb{R}^{1\times q}$。隐层到输出层的参数矩阵记为$\mathbf{W_2}\in \mathbb{R}^{q\times r}$，bias记为$\mathbf{b_2}\in \mathbb{R}^{1\times r}$。定义好变量后，前向传播的过程就可以表示如下：

$$\mathbf{z_1}=\mathbf{x}\mathbf{W_1}+\mathbf{b_1}$$
$$\mathbf{h}=\sigma(\mathbf{z_1})$$
$$\mathbf{z_2}=\mathbf{h}\mathbf{W_2}+\mathbf{b_2}$$
$$\mathbf{\hat y}=sofmax(\mathbf{z_2})$$

其中$\mathbf{z_1}$表示隐层的输入，$\mathbf{h}$表示隐层的输入经过激活函数后的输出，$\mathbf{z_2}$表示输出层的输入。激活函数定义为$\sigma(x)=\frac{1}{1+e^{-x}}$，又称为sigmoid激活函数。其他常见的激活函数还有tanh和ReLu。sigmoid函数的图像如下：
![sigmoid_function](http://cs231n.github.io/assets/nn1/sigmoid.jpeg)

一般如果是多分类的问题，我们希望输出层最后输出的是每个类别的概率。所以一般最后在输出层会套一个softmax函数，其定义为：

$$softmax(\mathbf{x})_i = \frac{e^{\mathbf{x}_i}} {\sum_j{e^{\mathbf{x}_j}}}$$

## 损失函数

损失函数衡量的是模型预测与真实值之间的差异。假设模型的输出是$\mathbf{\hat y}\in \mathbb{R}^{1\times r}$，真实的值是$\mathbf{y}\in \mathbb{R}^{1\times r}$，通常选择[交叉熵](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression)来度量预测和真实值的差异。交叉熵的定义如下：

$$CE(\mathbf{y},\mathbf{\hat y}) = -\sum_i{y_i\log{\hat y_i}}$$

上面定义了一个样本时，模型的损失函数，当有$n$个样本时，模型的损失函数$L$可以定义为：

$$L=\frac{1}{N}\sum_j{CE(\mathbf{y_j},\mathbf{\hat y_j})}$$


## 后向传播

接下来是神经网络推导中最精彩的部分了。如果把前向传播过程看成是从输入层->隐藏层->输出层的数据流动过程，那么后向传播则恰好是梯度(误差)从输出层->隐藏层->输入层的流动过程。

### 输出层的梯度计算
首先回顾下输出层的forward过程。输入是$\mathbf{z_2}$，输出是$\mathbf{\hat y}$，中间套了一个softmax函数。写成数学表达式为：
$$\mathbf{\hat y}=sofmax(\mathbf{z_2})$$
$$CE(\mathbf{y},\mathbf{\hat y}) = -\sum_i{y_i\log{\hat y_i}}$$
根据链式求导法则，$\frac{\partial CE}{\partial \mathbf{z_2}}=\frac{\partial CE}{\partial \mathbf{\hat y}}\frac{\partial \mathbf{\hat y}}{\partial \mathbf{z_2}}$。我们先来看softmax的求导。

首先，需要注意的是，这里是求一个向量相对于另一个向量的导数。在矩阵微积分中，是没有这样的定义的。我们知道实函数$f: x \rightarrow y$是可以求导的（前提是函数可导），而映射：$f: \mathbf{x}\in \mathbb{R}^n \rightarrow y$，也是可求导的，导数等于函数$f$对$\mathbf{x}$向量每个元素求导，即$\frac{\partial f}{\partial \mathbf{x}} = [\frac{\partial f}{\partial \mathbf{x_1}},\frac{\partial f}{\partial \mathbf{x_2}}... \frac{\partial f}{\partial \mathbf{x_n}}]$。而$\mathbf{\hat y}$的每个元素$\mathbf{\hat y_i}$可以表示为向量$\mathbf{z}$的函数:
$$\mathbf{\hat y_i}=\frac{e^{\mathbf{z}_i}} {\sum_j{e^{\mathbf{z}_j}}}$$

根据定义，$\mathbf{\hat y_i}$对$\mathbf{z}$的导数等于$\mathbf{\hat y_i}$对向量$\mathbf{z}$每个元素的导数：
$$\begin{equation}
\begin{split}
\frac{\partial \mathbf{\hat y_i}}{\partial \mathbf{z_j}}& = \frac{\partial}{\partial \mathbf{z_j}}\frac{e^{\mathbf{z_i}}}{\sum_k{e^{\mathbf{z_k}}}}\\
& =\begin{cases} \mathbf{\hat y_i}(1-\mathbf{\hat y_i}), & \text {if $i=j$} \\ \mathbf{\hat y_j}\mathbf{\hat y_i}, & \text{if $i\neq j$} \end{cases}\\
&
\end{split}
\end{equation}$$

最后，我们把softmax的求导应用到输出层的梯度计算中：

$$\begin{equation}
\begin{split}
\frac{\partial CE}{\partial \mathbf{z_j}}&=-\sum_i{\mathbf{y_i}\frac{\partial log\mathbf{\hat y_i}}{\partial \mathbf{z_j}}}\\
& =-\sum_j{\frac{\mathbf{y_i}}{\mathbf{\hat y_i}}\frac{\partial log\mathbf{\hat y_i}}{\partial\mathbf{z_j}}}\\
& =-\frac{\mathbf{y_j}}{\mathbf{\hat y_j}}\mathbf{\hat y_j}(1-\mathbf{\hat y_j})-\sum_{i\neq j}{\frac{\mathbf{y_i}}{\mathbf{\hat y_i}}(-\mathbf{\hat y_i}\mathbf{\hat y_j})}\\
& =\mathbf{y_j}\mathbf{\hat y_j}+\sum_{i\neq j}{\mathbf{y_i}\mathbf{\hat y_j}}-\mathbf{y_j}\\
& =\mathbf{\hat y_j}\left(\sum_i{\mathbf{y_i}}\right)-\mathbf{y_j}\\
& =\mathbf{\hat y_j}-\mathbf{y_j}
\end{split}
\end{equation}$$

因此，损失函数$CE(\mathbf{y},\mathbf{\hat y})$相对于输入$\mathbf{z_2}$的导数为：
$$\mathbf{\delta_1}=\frac{\partial CE(\mathbf{y},\mathbf{\hat y})}{\partial\mathbf{z_2}}=\mathbf{\hat y}-\mathbf{y}$$

经过一系列的求导后，发现结果非常的简单。

### 隐层的梯度计算
继续梯度的后向传播过程。从前向传播我们知道，$\mathbf{z_2}=\mathbf{hW_2}+\mathbf{b_2}$，接下来我们先求$CE$相对于$\mathbf{h}$的导数，再来求$CE$相对$\mathbf{W_2}$和$\mathbf{b_2}$的导数。
$$\begin{equation}\begin{split}
\frac{\partial CE}{\partial\mathbf{h}}&=\frac{\partial CE}{\partial\mathbf{z_2}}\frac{\partial\mathbf{z_2}}{\partial\mathbf{h}}\\
& =\mathbf{\delta_1}\frac{\partial\mathbf{z_2}}{\partial\mathbf{h}}
\end{split}\end{equation}
$$

同样，这里的$\mathbf{z_2}$和$\mathbf{h}$都是向量，我们可以采用求softmax梯度同样的技巧，按照每个元素来求导。
$$
\begin{equation}\begin{split}
\frac{\partial\mathbf{z_{2i}}}{\partial\mathbf{h_j}}&=\frac{\partial}{\partial\mathbf{h_j}}(\sum_{k=1}{\mathbf{h_kW_{ki}}})\\
& =\mathbf{W_{ji}}
&
\end{split}\end{equation}
$$

因此，
$$\frac{\partial\mathbf{z_2}}{\partial\mathbf{h}}=\mathbf{W^T}$$

综上，可以得到：

$$\mathbf{\delta}_2=\frac{\partial CE}{\partial\mathbf{h}}=\mathbf{\delta_1}\mathbf{W^T}$$

我们接着求$CE$相对于$\mathbf{W_2}$和$\mathbf{b_2}$的导数。这部分的推导我花了很长的时间，总算找到了一个比较容易理解的方法。下面一步步地来介绍。

首先，根据矩阵微积分定义，函数$f$对矩阵的求导，等于对$f$对矩阵每个元素求导，且函数$f$必须满足$f:\mathbb{R}^{m\times n}\rightarrow R$。于是我们得到：
$$\frac{\partial CE}{\partial\mathbf{W_2}}=\begin{bmatrix}
\frac{\partial CE}{\partial\mathbf{W}_{11}} & \cdots & \frac{\partial CE}{\partial\mathbf{W}_{1j}} & \cdots & \frac{\partial CE}{\partial\mathbf{W}_{1n}}\\
\vdots & & \vdots & & \vdots\\
\frac{\partial CE}{\partial\mathbf{W}_{m1}} & \cdots & \frac{\partial CE}{\partial\mathbf{W}_{mj}} & \cdots & \frac{\partial CE}{\partial\mathbf{W}_{mn}}\\
\end{bmatrix}
$$

我们先求$CE$对矩阵第$j$列的梯度，即$\frac{\partial CE}{\partial\mathbf{W}_{j}}$。
![](https://cl.ly/0x2B2G3h1O3j/%E7%BB%98%E5%9B%BE1.png)

从forward可以看出，参数$\mathbf{W_2}$的第j列的梯度只和输出层的第j个神经元有关。损失函数相对于神经元j的梯度是$\frac{\partial CE}{\partial\mathbf{z_j}}=d\mathbf{z_j}$，而根据forward过程，神经元$\mathbf{z_j}$的输入等于隐层单元乘以相应的边权重的累加，即$\mathbf{z_j}=\mathbf{h}_1\mathbf{W}_{1j}+\mathbf{h}_2\mathbf{W}_{2j}+\dots+\mathbf{h}_m\mathbf{W}_{nj}$，所以损失函数相对于$\mathbf{W}_2$第$j$列的每个元素的导数可以表示成向量的形式：
$$\frac{\partial CE}{\partial\mathbf{W}_j}=\begin{bmatrix}
\frac{\partial CE}{\partial\mathbf{z}_j}\frac{\partial\mathbf{z}_j}{\partial\mathbf{W}_{1j}}\\
\frac{\partial CE}{\partial\mathbf{z}_j}\frac{\partial\mathbf{z}_j}{\partial\mathbf{W}_{2j}}\\
\vdots\\
\frac{\partial CE}{\partial\mathbf{z}_j}\frac{\partial\mathbf{z}_j}{\partial\mathbf{W}_{mj}}\\
&
\end{bmatrix}=\begin{bmatrix}
\mathbf{h}_1 d\mathbf{z_j}\\
\mathbf{h}_2 d\mathbf{z_j}\\
\vdots\\
\mathbf{h}_n d\mathbf{z_j}
\end{bmatrix}
$$

写出了$\mathbf{W}_2$的每列的梯度后，我们就可以写出损失函数对整个矩阵的梯度了，
$$\frac{\partial CE}{\partial\mathbf{W}_2}=\begin{bmatrix}
\mathbf{h}_1 d\mathbf{z_1} & \mathbf{h}_1 d\mathbf{z_2} & \cdots & \mathbf{h}_1 d\mathbf{z_n}\\
\mathbf{h}_2 d\mathbf{z_1} & \mathbf{h}_2 d\mathbf{z_2} & \cdots & \mathbf{h}_2 d\mathbf{z_n}\\
\vdots & \vdots & \cdots &\vdots\\
\mathbf{h}_m d\mathbf{z_1} & \mathbf{h}_m d\mathbf{z_2} & \cdots & \mathbf{h}_m d\mathbf{z_n}\\
\end{bmatrix}=\mathbf{h}^T \bf{\delta}_1$$

注意$\mathbf{h}$和$\bf{\delta}_1$都是行向量，这里是两个向量的外积，结果是一个矩阵。维数正好和参数矩阵$\mathbf{W}_2$相同。在写程序的过程中我们也可以通过梯度的维数来校验程序是否正确，因为不管是损失函数相对于矩阵还是向量的梯度，其维数肯定是要和原矩阵或向量保持一致。

最后，还需要求损失函数相对于偏置向量的导数，这个比较简单，结果如下：
$$\frac{\partial CE}{\partial\mathbf{b}_2}=\frac{\partial CE}{\partial\mathbf{z}_2}\frac{\partial\mathbf{z}_2}{\partial\mathbf{b}_2}=\mathbf{\delta_1}$$

到这里基本上最难的部分已经算讲完了，剩下的推导过程是类似的。继续沿着backward的方向，接下来需要求损失函数对于隐层的输入$z_1$的梯度。同样应用链式求导法则：
$$\mathbf{\delta_3}=\frac{\partial CE}{\partial\mathbf{z}_1}=\frac{\partial CE}{\partial\mathbf{h}}\frac{\partial\mathbf{h}}{\partial\mathbf{z}_1}=\mathbf{\delta_2}\mathbf{\sigma'(\mathbf{z}_1)}=\mathbf{\delta_2}\mathbf{\sigma(\mathbf{z}_1)}(1-\mathbf{\sigma(\mathbf{z}_1)})$$

知道了$CE$相对于$\mathbf{z}_1$的梯度后，求$\mathbf{W}_1$和$\mathbf{b}_1$的梯度和之前求$\mathbf{W}_2$及$\mathbf{b}_2$的完全类似。
$$\frac{\partial CE}{\partial\mathbf{W}_1}=\frac{\partial CE}{\partial\mathbf{z}_1}\frac{\partial\mathbf{z}_1}{\partial\mathbf{W}_1}=\mathbf{x}^T\mathbf{\delta_3}$$

$$\frac{\partial CE}{\partial\mathbf{b}_1}=\mathbf{\delta_3}$$
